{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d39ef0-a1a1-487a-8d1e-8d6611c55569",
   "metadata": {},
   "source": [
    "### Building Multi-stage Reasoning Systems with LangChain\n",
    "\n",
    "#### Multi-stage reasoning systems\n",
    "\n",
    "In this notebook we're going to create two AI systems:\n",
    "- The first, code named `JekyllHyde` will be a prototype AI self-commenting-and-moderating tool that will create new reaction comments to a piece of text with one LLM and use another LLM to critique those comments and flag them if they are negative. To build this we will walk through the steps needed to construct prompts and chains, as well as multiple LLM Chains that take multiple inputs, both from the previous LLM and external. \n",
    "- The second system, codenamed `DaScie` (pronounced \"dae-see\") will take the form of an LLM-based agent that will be tasked with performing data science tasks on data that will be stored in a vector database using ChromaDB. We will use LangChain agents as well as the ChromaDB library, as well as the Pandas Dataframe Agent and python REPL (Read-Eval-Print Loop) tool.\n",
    "\n",
    "----\n",
    "\n",
    "#### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Build prompt template and create new prompts with different inputs\n",
    "2. Create basic LLM chains to connect prompts and LLMs.\n",
    "3. Construct sequential chains of multiple `LLMChains` to perform multi-stage reasoning analysis. \n",
    "4. Use langchain agents to build semi-automated systems with an LLM-centric agent to perform internet searches and dataset analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f312cf-f52f-44e7-b084-d6d4c02ad26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia==1.4.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: google-search-results==2.4.2 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: better-profanity==0.7.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: langchain in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (0.0.339)\n",
      "Requirement already satisfied: langchain_experimental in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (0.0.42)\n",
      "Requirement already satisfied: openai in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (1.3.3)\n",
      "Requirement already satisfied: pydantic==1.10.9 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (1.10.9)\n",
      "Requirement already satisfied: beautifulsoup4 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from wikipedia==1.4.0) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from wikipedia==1.4.0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from pydantic==1.10.9) (4.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (0.6.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (0.0.65)\n",
      "Requirement already satisfied: numpy<2,>=1 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (1.26.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from openai) (0.25.1)\n",
      "Requirement already satisfied: tqdm>4 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.5)\n",
      "Requirement already satisfied: packaging>=17.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /linux-data/miniconda3/envs/llm-edx-course/lib/python3.10/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia==1.4.0 google-search-results==2.4.2 better-profanity==0.7.0 langchain langchain_experimental openai pydantic==1.10.9\n",
    "cache_dir='cache_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c3c498-3fd8-4285-b98d-3a7d189531cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75798b13-7fe1-48a4-9886-7bfe3fe18979",
   "metadata": {},
   "source": [
    "#### JekyllHyde - A self moderating system for social media\n",
    "\n",
    "In this section we will build an AI system that consists of two LLMs. `Jekyll` will be an LLM designed to read in a social media post and create a new comment. However, `Jekyll` can be moody at times so there will always be a chance that it creates a negative-sentiment comment... we need to make sure we filter those out. Luckily, that is the role of `Hyde`, the other LLM that will watch what `Jekyll` says and flag any negative comments to be removed. \n",
    "\n",
    "#### Step 1 - Letting Jekyll Speak\n",
    "\n",
    "##### Building the Jekyll Prompt\n",
    "\n",
    "To build `Jekyll` we will need it to be able to read in the social media post and respond as a commenter. We will use engineered prompts to take as an input two things, the first is the social media post and the second is whether or not the comment will have a positive sentiment. We'll use a random number generator to create a chance of the flag to be positive or negative in `Jekyll's` response.\n",
    "\n",
    "Let's start with the prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0662a91b-4649-491d-a2a4-9f35fd2ed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our template for Jekyll will instruct it on how it should respond, and what variables (using the {text} syntax) it should use.\n",
    "jekyll_template = \"\"\"\n",
    "You are a social media post commenter, you will respond to the following post with a {sentiment} response. \n",
    "Post:\" {social_post}\"\n",
    "Comment: \n",
    "\"\"\"\n",
    "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
    "jekyll_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\", \"social_post\"],\n",
    "    template=jekyll_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "226ae9df-ec2a-4ba3-8e7b-5f249cc00a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay now that's ready we need to make the randomized sentiment\n",
    "random_sentiment = \"nice\"\n",
    "if np.random.rand() < 0.3:\n",
    "    random_sentiment = \"mean\"\n",
    "# We'll also need our social media post:\n",
    "social_post = \"I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
    "\n",
    "# Let's create the prompt and print it out, this will be given to the LLM.\n",
    "jekyll_prompt = jekyll_prompt_template.format(\n",
    "    sentiment=random_sentiment, social_post=social_post\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1297b-6d06-4c33-b483-33a1e1c91ec1",
   "metadata": {},
   "source": [
    "#### Step 2 - Giving Jekyll a brain!\n",
    "##### Building the Jekyll LLM \n",
    "\n",
    "Note: We provide an option for you to use either Hugging Face or OpenAI. If you continue with Hugging Face, the notebook execution will take a long time (up to 10 mins each cell). If you don't mind using OpenAI, following the next markdown cell for API key generation instructions. \n",
    "\n",
    "#### To interact with LLMs in LangChain we need the following modules loaded\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipelineFor OpenAI,  \n",
    "\n",
    "We will use their GPT-3 model: `text-babbage-001` as our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115e8360-9679-40d2-b8f7-c3c51d4092e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To interact with LLMs in LangChain we need the following modules loaded\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from better_profanity import profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031530ee-690b-4590-a707-caf70f4d5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll said: That's great to hear! It's so rewarding when you learn something new and the instructors are there to help. Keep up the great work! #AI #Databricks\n",
      "ORIGINAL COMMENT: That's great to hear! It's so rewarding when you learn something new and the instructors are there to help. Keep up the great work! #AI #Databricks\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Model\n",
    "jekyll_llm = OpenAI(model=\"text-davinci-003\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "jekyll_chain = LLMChain(\n",
    "    llm=jekyll_llm,\n",
    "    prompt=jekyll_prompt_template,\n",
    "    output_key=\"jekyll_said\",\n",
    "    verbose=False,\n",
    ") \n",
    "\n",
    "# To run our chain we use the .run() command and input our variables as a dict\n",
    "jekyll_said = jekyll_chain.run(\n",
    "    {\"sentiment\": random_sentiment, \"social_post\": social_post}\n",
    ")\n",
    "\n",
    "# Before printing what Jekyll said, let's clean it up:\n",
    "cleaned_jekyll_said = profanity.censor(jekyll_said)\n",
    "print(f\"Jekyll said: {cleaned_jekyll_said}\")\n",
    "print(f\"ORIGINAL COMMENT: {jekyll_said}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d983018-0a54-4ad3-aad7-936df9bf6176",
   "metadata": {},
   "source": [
    "#### Step 4 - Time for Jekyll to Hyde\n",
    "##### Building the second chain for our Hyde moderator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd5b4ba-24bb-4acb-9083-94f086d28467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyde says: â¤ï¸ That's great to hear! It's so rewarding when you learn something new and the instructors are there to help. Keep up the great work! #AI #Databricks â¤ï¸\n"
     ]
    }
   ],
   "source": [
    "# 1 We will build the prompt template\n",
    "# Our template for Hyde will take Jekyll's comment and do some sentiment analysis.\n",
    "hyde_template = \"\"\"\n",
    "You are Hyde, the moderator of an online forum.\n",
    "You are strict and will not tolerate any negative comments. \n",
    "You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols, \n",
    "post that and scold the user but if it seems nice respond with a love sentiment\n",
    "Original comment: {jekyll_said}\n",
    "Your Comment:\n",
    "\"\"\"\n",
    "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
    "hyde_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"jekyll_said\"],\n",
    "    template=hyde_template,\n",
    ")\n",
    "\n",
    "#####################################\n",
    "# 2 We connect an LLM for Hyde\n",
    "\n",
    "hyde_llm = jekyll_llm\n",
    "\n",
    "#####################################\n",
    "# 3 We build the chain for Hyde\n",
    "hyde_chain = LLMChain(\n",
    "    llm=hyde_llm, prompt=hyde_prompt_template, verbose=False\n",
    ")  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
    "\n",
    "#####################################\n",
    "# 4 Let's run the chain with what Jekyll last said\n",
    "# To run our chain we use the .run() command and input our variables as a dict\n",
    "hyde_says = hyde_chain.run({\"jekyll_said\": jekyll_said})\n",
    "# Let's see what hyde said...\n",
    "print(f\"Hyde says: {hyde_says}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8f122-c66f-47ed-824d-e6068d8b8cab",
   "metadata": {},
   "source": [
    "#### Step 5 - Creating `JekyllHyde`\n",
    "\n",
    "##### Building our first Sequencial Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344c417d-edd9-4953-8df7-c971b94dcf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ðŸ˜Šâ¤ï¸ That's great to hear! It's always exciting to learn something new, and it sounds like you're having a great time. Keep it up and good luck! #AI #Databricks\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "jekyllhyde_chain = SequentialChain(\n",
    "    chains = [jekyll_chain, hyde_chain],\n",
    "    input_variables = ['sentiment', 'social_post'],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "jekyllhyde_chain.run({\"sentiment\": random_sentiment, \"social_post\": social_post})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2a7ab-1727-4512-8098-f53640cf3ae0",
   "metadata": {},
   "source": [
    "#### Step 5 - Creating `JekyllHyde`\n",
    "\n",
    "##### Building our first Sequencial Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "710a05c1-1153-42d1-8cb8-3fe1c62dc6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ðŸ˜ŠðŸ˜ŠðŸ˜Š'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "jekyllhyde_chain = SequentialChain(\n",
    "    chains = [jekyll_chain, hyde_chain],\n",
    "    input_variables = ['sentiment', 'social_post'],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "jekyllhyde_chain.run({\"sentiment\": random_sentiment, \"social_post\": social_post})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
