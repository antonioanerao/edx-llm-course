{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784b396e-5b8a-4494-a8dc-06de5c6ff068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: datasets in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: deepspeed==0.9.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (0.9.1)\n",
      "Requirement already satisfied: py-cpuinfo==9.0.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (9.0.0)\n",
      "Requirement already satisfied: transformers in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: accelerate in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (0.24.1)\n",
      "Requirement already satisfied: tensorboard in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: hjson in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from deepspeed==0.9.1) (3.1.0)\n",
      "Requirement already satisfied: ninja in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from deepspeed==0.9.1) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from deepspeed==0.9.1) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from deepspeed==0.9.1) (23.2)\n",
      "Requirement already satisfied: psutil in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from deepspeed==0.9.1) (5.9.6)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from deepspeed==0.9.1) (1.10.13)\n",
      "Requirement already satisfied: torch in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from deepspeed==0.9.1) (2.1.1)\n",
      "Requirement already satisfied: tqdm in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from deepspeed==0.9.1) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: filelock in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (1.59.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (3.5.1)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from torch->deepspeed==0.9.1) (1.11.1)\n",
      "Requirement already satisfied: networkx in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from torch->deepspeed==0.9.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from torch->deepspeed==0.9.1) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /linux-data/miniconda3/envs/linuxtips-llm/lib/python3.10/site-packages (from sympy->torch->deepspeed==0.9.1) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas datasets deepspeed==0.9.1 py-cpuinfo==9.0.0 transformers datasets accelerate tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461ce5ce-9d13-4c09-ab6b-8d090deb1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "import transformers as tr\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a50f83-698c-45de-bc6e-5aff237a9f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user is antonio\n",
      "Current working dir is /linux-data/llm-course\n"
     ]
    }
   ],
   "source": [
    "# Define um diretório de cache\n",
    "cache_dir = 'cache_dir'\n",
    "username = os.environ['USER']\n",
    "working_dir = os.environ['PWD']\n",
    "\n",
    "print(f\"Current user is {username}\")\n",
    "print(f\"Current working dir is {working_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab1b9026-428d-4084-91cd-64011cc01208",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29faf585-1737-4123-be9a-a59b7c096411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 1\n",
      "GPU 0: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "# Verifica se há uma GPU disponível e define o dispositivo (GPU ou CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "if num_gpus > 0:\n",
    "    print(f\"GPUs available: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "else:\n",
    "    print(\"There is no GPU available. Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca2621b-55a4-4ca2-91cc-c4fe1fe2e9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpvf5_phaq\n"
     ]
    }
   ],
   "source": [
    "# Creating a local temporary directory on the Driver. \n",
    "# This will serve as a root directory for the intermediate model checkpoints created during the training process. \n",
    "# The final model will be persisted to DBFS.\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "local_training_root = tmpdir.name\n",
    "\n",
    "print(local_training_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f20599-9eb4-46aa-b480-f561f1a52df6",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "\n",
    "#### Step 1 - Data Preparation\n",
    "\n",
    "The first step of the fine-tuning process is to identify a specific task and supporting dataset. In this notebook, we will consider the specific task to be classifying movie reviews. This idea is generally simple task where a movie review is provided as plain-text and we would like to determine whether or not the review was positive or negative.\n",
    "\n",
    "The [IMDB dataset](https://huggingface.co/datasets/imdb) can be leveraged as a supporting dataset for this task. The dataset conveniently provides both a training and testing dataset with labeled binary sentiments, as well as a dataset of unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa0c29a-b3dc-4bb7-95e4-ab6c2d08c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_ds = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22496e0b-f065-45af-8573-420e89df5216",
   "metadata": {},
   "source": [
    "#### Step 2 - Select pre-trained model\n",
    "\n",
    "The next step of the fine-tuning process is to select a pre-trained model. We will consider using the [T5](https://huggingface.co/docs/transformers/model_doc/t5) [[paper]](https://arxiv.org/pdf/1910.10683.pdf) family of models for our fine-tuning purposes. The T5 models are text-to-text transformers that have been trained on a multi-task mixture of unsupervised and supervised tasks. They are well suited for tasks such as summarization, translation, text classification, question answering, and more.\n",
    "\n",
    "The `t5-small` version of the T5 models has 60 million parameters. This slimmed down version will be sufficient for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f96b9a-244e-4ac8-914c-d4e254d51381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77641ffa-38dc-494f-873b-e868dcb12c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer that was used for the t5-small model\n",
    "\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71282ebe-77d3-4087-b2b8-5082e76c9ba8",
   "metadata": {},
   "source": [
    "As mentioned above, the IMDB dataset is a binary sentiment dataset. Its labels therefore are encoded as `(-1 - unknown; 0 - negative; 1 - positive)` values. In order to use this dataset with a text-to-text model like T5, the label set needs to be represented as a string. There are a number of ways to accomplish this. Here, we will simply translate each label id to its corresponding string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b7617a5-a8ed-4402-99ad-056628a08cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(tokenizer: tr.models.t5.tokenization_t5_fast.T5TokenizerFast, label_map: dict) -> callable:\n",
    "    \"\"\"\n",
    "    Given a `tokenizer` this closure will iterate through `x` and return the result of `apply()`.\n",
    "    This function is mapped to a dataset and returned with ids and attention mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply(x) -> tr.tokenization_utils_base.BatchEncoding:\n",
    "        \"\"\"From a formatted dataset `x` a batch encoding `token_res` is created.\"\"\"\n",
    "        target_labels = [label_map[y] for y in x[\"label\"]]\n",
    "        token_res = tokenizer(\n",
    "            x[\"text\"],\n",
    "            text_target=target_labels,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "        )\n",
    "        return token_res\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "imdb_label_lookup = {0: \"negative\", 1: \"positive\", -1: \"unknown\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd20e6a-e7ea-4135-82ec-e497fd1e09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_to_tokens = to_tokens(tokenizer, imdb_label_lookup)\n",
    "tokenized_dataset = imdb_ds.map(imdb_to_tokens, batched=True, remove_columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48daab34-66c3-41c3-ad36-a05b5bdcd0d7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Step 3 - Setup Training\n",
    "\n",
    "The model training process is highly configurable. The [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) class effectively exposes the configurable aspects of the process allowing one to customize them accordingly. Here, we will focus on setting up a training process that performs a single epoch of training with a batch size of 16. We will also leverage `adamw_torch` as the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aefd8e3-3ed0-4be1-bcfa-cea2d1b123b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'test-trainer'\n",
    "local_checkpoint_path = os.path.join(local_training_root, checkpoint_name)\n",
    "\n",
    "training_args = tr.TrainingArguments(\n",
    "    local_checkpoint_path,\n",
    "    num_train_epochs = 1, # default number of epochs to train is 3\n",
    "    per_device_train_batch_size = 16,\n",
    "    optim = 'adamw_torch',\n",
    "    report_to = ['tensorboard']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f781f69f-7aee-44d6-9970-3852d23ebc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tr.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    cache_dir = cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3c78654-b083-40e8-bffd-100a3155857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to assist the trainer in batching the data\n",
    "\n",
    "data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = tr.Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset = tokenized_dataset['test'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebef469-52c4-4f89-bbef-67c0dee7458a",
   "metadata": {},
   "source": [
    "Before starting the training process, let's turn on Tensorboard. This will allow us to monitor the training process as checkpoint logs are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de1d40c-d368-4105-8e1e-abfdbc76d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_display_dir = f\"{local_checkpoint_path}/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94a15441-f719-4ce7-960f-79f1889ae5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{tensorboard_display_dir}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc2cbd-8fe8-4367-9bb2-53225b752d79",
   "metadata": {},
   "source": [
    "#### Fine-Tuning do modelo t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cfc3292-7831-459f-bc7d-334cb237ce33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 07:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fa0445b-791c-47dc-a52a-8d8a0aadff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the fine-tuned model to DBFS\n",
    "\n",
    "final_model_path = f\"{working_dir}/llm04_fine_tuning/{checkpoint_name}\"\n",
    "trainer.save_model(output_dir = final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb5a19-70fd-4585-bb60-46ec7ddb7a81",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "787024c7-f6c3-4504-9fdc-4895843f8213",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = tr.AutoModelForSeq2SeqLM.from_pretrained(final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5df68e40-d3cd-4dd3-8c36-ea0fa7862955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T20:00:55.050681Z",
     "iopub.status.busy": "2023-11-24T20:00:55.050511Z",
     "iopub.status.idle": "2023-11-24T20:00:55.150939Z",
     "shell.execute_reply": "2023-11-24T20:00:55.150570Z",
     "shell.execute_reply.started": "2023-11-24T20:00:55.050668Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews = [\n",
    "\"\"\" 'Despicable Me' is a cute and funny movie, but the plot is predictable and the characters are not very well-developed. Overall, it's a good movie for kids, but adults might find it a bit boring.\"\"\",\n",
    "\"\"\" 'The Batman' is a dark and gritty take on the Caped Crusader, starring Robert Pattinson as Bruce Wayne. The film is a well-made crime thriller with strong performances and visuals, but it may be too slow-paced and violent for some viewers.\"\"\",\n",
    "\"\"\" The Phantom Menace is a visually stunning film with some great action sequences, but the plot is slow-paced and the dialogue is often wooden. It is a mixed bag that will appeal to some fans of the Star Wars franchise, but may disappoint others.\"\"\",\n",
    "\"\"\" I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\"\"\",\n",
    "\"\"\" What the heck was that? Superb. Disgusting. No idea if I like it or hate it \"\"\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    reviews,\n",
    "    return_tensors = 'pt',\n",
    "    truncation = True,\n",
    "    padding = True\n",
    ")\n",
    "pred = fine_tuned_model.generate(\n",
    "    input_ids = inputs['input_ids'], \n",
    "    attention_mask = inputs['attention_mask'],\n",
    "    max_new_tokens = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e65ac5b3-f340-485f-a9c8-a5f11769ccfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T20:00:52.761505Z",
     "iopub.status.busy": "2023-11-24T20:00:52.761342Z",
     "iopub.status.idle": "2023-11-24T20:00:52.781429Z",
     "shell.execute_reply": "2023-11-24T20:00:52.781047Z",
     "shell.execute_reply.started": "2023-11-24T20:00:52.761492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Despicable Me' is a cute and funny movie, bu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'The Batman' is a dark and gritty take on the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Phantom Menace is a visually stunning fil...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm not sure if The Matrix and the two sequel...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What the heck was that? Superb. Disgusting. N...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review classification\n",
       "0   'Despicable Me' is a cute and funny movie, bu...       positive\n",
       "1   'The Batman' is a dark and gritty take on the...       positive\n",
       "2   The Phantom Menace is a visually stunning fil...       positive\n",
       "3   I'm not sure if The Matrix and the two sequel...       negative\n",
       "4   What the heck was that? Superb. Disgusting. N...       negative"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf = pd.DataFrame(zip(reviews, tokenizer.batch_decode(pred, skip_special_tokens = True)), columns = ['review', 'classification'])\n",
    "display(pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
